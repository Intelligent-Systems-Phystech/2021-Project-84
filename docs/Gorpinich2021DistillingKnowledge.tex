\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\newcommand{\hdir}{.}

\input{math_symbols}

\begin{document}

\title
    [Оптимизация параметров модели на основе дистилляции знаний] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Регуляризация траектории оптимизации параметров модели глубокого обучения на основе дистилляции знаний}
\author
    [М.~Горпинич] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {М.~Горпинич, О.\,Ю.~Бахтеев, В.\,В.~Стрижов} % основной список авторов, выводимый в оглавление
    [М.~Горпинич, О.\,Ю.~Бахтеев, В.\,В.~Стрижов] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {gorpinich4@gmail.com; bakhteev@phystech.edu;  strijov@ccas.ru}
%\thanks
%    {Работа выполнена при
%     частичной
%     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
%\organization
%    {$^1$Организация, адрес; $^2$Организация, адрес}
\abstract
    {Исследуется задача оптимизации параметров модели глубокого обучения. Предлагается обобщение методов дистилляции, заключающееся в градиентной оптимизации гиперпараметров. На первом уровне оптимизируются параметры модели, на втором --- гиперпараметры, задающие вид оптимизационной задачи. Исследуются свойства оптимизационной задачи и различные виды оператора оптимизации. Предложенное обобщение оптимизации позволяет производить дистилляцию модели с лучшими эксплуатационными характеристиками и за меньшее количество итераций оптимизации. Иллюстрировать применение комбинации данных подходов предлагается с помощью вычислительного эксперимента на выборке CIFAR-10.
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {}
}

%данные поля заполняются редакцией журнала
\doi{}
\receivedRus{}
\receivedEng{}

\maketitle
\linenumbers

\section{Введение}
В работе рассматривается задача оптимизации моделей глубоких нейросетей. Данная задача требует значительных вычислительных мощностей и является затратной по времени. В данной работе предлагается метод оптимизации, позволяющий улучшить эксплуатационные характеристики модели, а также ускорить ее сходимость к точке оптимума.

Предлагается обобщение метода оптимизации на основе дистилляции знаний. Рассматривается \textit{модель-учитель} более сложной структуры, которая была обучена на выборке. Модель более простой структуры предлагается оптимизировать путем переноса знаний модели учителя на более простую модель, называемую \textit{моделью-учеником}, при этом ее качество будет выше по сравнению с качеством, полученным после оптимизации на той же выборке. Примером применения данного подхода является \cite{journals/corr/HintonVD15}. В работе \cite{conf/cvpr/PassalisTT20} предложен подход к дистилляции знаний, позволяющий переносить знания на модель с архитектурой, значительно отличающейся от архитектуры модели-учителя.

Предлагается представление задачи в виде двухуровневой оптимизации. На первом уровне оптимизации происходит оптимизация параметров модели, на втором уровне --- ее гиперпараметров. Данный подход описан в работах \cite{journals/corr/LuketinaBR15, journals/anor/BakhteevS20, journals/corr/MaclaurinDA15}. В работе \cite{journals/corr/LuketinaBR15} рассматривается жадный градиентный метод оптимизации гиперпараметров, в работе \cite{journals/anor/BakhteevS20} сравниваются различные градиентные методы оптимизации гиперпараметров, а также метод случайного поиска.

В работе рассматривается вид задачи оптимизации, а также различные виды оператора оптимизации. Данный подход с использованием нейросети LSTM описан в работе \cite{journals/corr/AndrychowiczDGH16}. Вычислительный эксперимент проводится на выборке изображений CIFAR-10.

\section{Постановка задачи}
Решается задача классификации вида:

\begin{equation} \label{eq:data}
    \fD = \{(\bx_i, y_i)\}_{i=1}^{m},\; \bx_i \in \bbR^n,\; y_i \in \bbY = \{1, \dots, K\},
\end{equation}

\noindent
где $y_i$ — это класс объекта, также будем обозначать $\by_i$ вектором вероятности для
класса $y_i$.

Разобьем выборку следующим образом:

\begin{equation} \label{eq:split}
    \fD = \fD_\text{train} \sqcup \fD_\text{val}.
\end{equation}

Подвыборку $\fD_\text{train}$ будем использовать для оптимизации параметров модели, а подвыборку $\fD_\text{val}$ --- для оптимизации гиперпараметров.

В качестве внешнего критерия качества рассматривается доля правильных ответов:

\begin{equation} \label{eq:accuracy}
    accuracy = \frac{1}{m}\sum\limits_{i=1}^m [\bg(\bx_i, \bw) = y_i],
\end{equation}

\noindent
где $\bg$ --- параметрическая модель классификации с параметрами $\bw$.

Пусть задана модель учителя $\bff$. Функция потерь $\cL_\text{train}$, в которой учитывается перенос информации от модели учителя $\bff$ к модели ученика $\bg$, имеет следующий вид:

\begin{equation} \label{eq:l_train}
    \cL_\text{train}(\bw, \bh) = -\sum\limits_{(\bx, y) \in \fD_\text{train}}\underbrace{\sum\limits_{k=1}^{K}y^k\log \bg(\bx, \bw)|_{T=1}}_{\text{исходная функция потерь}} - \beta\sum\limits_{(\bx, y) \in \fD_\text{train}}\underbrace{\sum\limits_{k=1}^{K}\bff(\bx)|_{T=T_0}\log \bg(\bx, \bw)|_{T=T_0}}_{\text{слагаемое дистилляции}},
\end{equation}

\noindent
где $T$ --- параметр температуры. Параметр температуры $T$ имеет следующие свойства:

\begin{itemize}
    \item[1)] при $T \rightarrow 0$ получаем вектор, в котором один из классов имеет единичную вероятность;
    \item[2)] при $T \rightarrow \infty$ получаем равновероятные классы.
\end{itemize}

\noindent
Выражение $\cdot |_{T=t}$ обозначает, что параметр температуры $T$ в предыдущей функции равняется $t$.

Зададим множество гиперпараметров $\bh$ как вектор, состоящий из температуры и коэффициента перед слагаемым дистилляции:

\[\bh = [\beta, T].\]

Итоговая оптимизационная задача выглядит следующим образом:

\begin{equation} \label{eq:opt_hyp}
    \hat{\bh} = \argmax\limits_{\bh \in \bbR^2} \cL_\text{val}(\hat{\bw}, \bh),
\end{equation}

\begin{equation} \label{eq:opt_param}
    \hat{\bw} = \argmin\limits_{\bw \in \bbR^s} \cL_\text{train}(\bw, \bh),
\end{equation}

\noindent
где функция $\cL_\text{val}$ определяется следующим образом: 
 \begin{equation} \label{eq:l_val}
     \cL_\text{val}(\bw, \bh) = \sum\limits_{(\bx, y) \in \fD_\text{val}}\sum\limits_{k=1}^{K}y^k\log \bg(\bx, \bw)|_{T=1}.
 \end{equation}

\textbf{Определение 1.} Назовем оператором оптимизации алгоритм $U$ выбора вектора параметров $\bw^\prime$ по параметрам предыдущего шага $\bw$:

\begin{equation*}
    \bw^\prime = U(\bw).
\end{equation*}

\subsection{Градиентные методы оптимизации параметров дистилляции модели}

Примером оператора оптимизации выступает оператор градиентного спуска:

\begin{equation} \label{eq:operator}
    U(\bw, \bh) = \bw - \gamma\nabla\cL_\text{train}(\bw, \bh),
\end{equation}

\noindent
где $\bh$ --- совокупность гиперпараметров модели, $\gamma$ — длина шага градиентного спуска.

Оптимизируем параметры $\bw$ при помощи $\eta$ шагов градиентного спуска:

\begin{equation} \label{eq:oper_superp}
    \hat{\bw} = U \circ U \circ \dots \circ U(\bw_0, \bh) = U^\eta(\bw_0, \bh),
\end{equation}

\noindent
где $\bw_0$ --- начальное значение вектора параметров $\bw$.

Переопределим задачу минимизации согласно определению оператора $U$:

\begin{equation} \label{eq:hyp_oper}
    \hat{\bh} = \argmax\limits_{\bh \in \bbR^2} \cL_\text{val}(U^\eta(\bw_0, \bh)).
\end{equation}

Решим задачу \eqref{eq:hyp_oper} используя градиентный метод. Схема оптимизации гиперпараметров:

\begin{enumerate}
    \item Для каждого $i = \overline{0, l}$, где $l$ --- количество итераций, используемых для оптимизации гиперпараметров:
    \item Решим задачу \eqref{eq:hyp_oper} и получим новое значение гиперпараметров $\bh^\prime$.
    \item Положим $\bh = \bh^\prime$.
\end{enumerate}

Будем обновлять гиперпараметры $\bh$, используя метод градиентного спуска, который зависит только от значений параметров $\bw$ на предыдущем шаге. На каждой итерации получим следующее значение гиперпараметров:

\begin{equation} \label{eq:hyp_alg}
    \bh^\prime = \bh - \gamma_{\bh}\nabla_{\bh}\cL_\text{val}(U(\bw, \bh), \bh) = \bh - \gamma_{\bh}\nabla_{\bh}\cL_\text{val}(\bw - \gamma\nabla\cL_\text{train}(\bw, \bh), \bh).
\end{equation}

\paragraph{Название параграфа}
Разделы и~параграфы, за исключением списков литературы, нумеруются.

\section{Заключение}
Желательно, чтобы этот раздел был, причём он не~должен дословно повторять аннотацию.
Обычно здесь отмечают, каких результатов удалось добиться, какие проблемы остались открытыми.

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/

\bibliographystyle{bibstyle.bst}
\bibliography{bibliography.bib}


%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/.

\end{document}
