{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "import torch as t \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "import json\n",
    "import hyperparams\n",
    "from importlib import reload\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(12,9)\n",
    "plt.rcParams['font.size']= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_num = 10 \n",
    "total_iteration_num= 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "teacher_train_size, student_train_size, student_validation_size, test_size = 200, 15, 15, 200\n",
    "total_size =  sum([teacher_train_size, student_train_size, student_validation_size, test_size])\n",
    "eps = .5\n",
    "\"\"\"\n",
    "Y генерируется как:\n",
    "    sign (x1 * x2) + шум\n",
    "    \n",
    "Признаки для учителя:\n",
    "    x1; x2; индикатор, что x1 >0 и x2>0 \n",
    "    Последний признак коррелирует с Y, но не покрывает его\n",
    "    \n",
    "Признаки для ученика:\n",
    "    x1; x2; индикатор, что x1 >0 и x2>0 \n",
    "    Последний признак отличается от Y из-за шума\n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "# полные данные\n",
    "x_big = np.random.randn(total_size, 2)\n",
    "y_big = (np.sign(x_big[:,0]*x_big[:,1]+np.random.randn(total_size)*eps)+1)//2\n",
    "x_big = np.vstack([x_big[:,0], x_big[:,1],  (np.sign(x_big[:,0])+np.sign(x_big[:,1]) >0.1)*1.0, \n",
    "                   x_big[:,0]*x_big[:,1]]).T\n",
    "\n",
    "# обучение учителя. NB: не пересекается с выборкой для обучения ученика, так проще подобрать параметры\n",
    "x_ttrain = t.tensor(x_big[:teacher_train_size], dtype=t.float32)\n",
    "y_ttrain = t.tensor(y_big[:teacher_train_size], dtype=t.long)\n",
    "\n",
    "\n",
    "# обучение ученика\n",
    "x_train = t.tensor(x_big[teacher_train_size:teacher_train_size+student_train_size], dtype=t.float32)\n",
    "y_train = t.tensor(y_big[teacher_train_size:teacher_train_size+student_train_size], dtype=t.long)\n",
    "\n",
    "# валидация, в этом ноутбуке не используется\n",
    "x_val = t.tensor(x_big[teacher_train_size+student_train_size:teacher_train_size+student_train_size+student_validation_size], \n",
    "                 dtype=t.float32)\n",
    "y_val = t.tensor(y_big[teacher_train_size+student_train_size:teacher_train_size+student_train_size+student_validation_size], dtype=t.long)\n",
    "\n",
    "\n",
    "x_test = t.tensor(x_big[-test_size:], dtype=t.float32)\n",
    "y_test = t.tensor(y_big[-test_size:], dtype=t.long)\n",
    "\n",
    "plt.scatter(x_big[y_big==0,0], x_big[y_big==0,1])\n",
    "plt.scatter(x_big[y_big==1,0], x_big[y_big==1,1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x_train[y_train==0,0], x_train[y_train==0,1])\n",
    "plt.scatter(x_train[y_train==1,0], x_train[y_train==1,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(t.nn.Module):\n",
    "    def __init__(self, idx):\n",
    "        t.nn.Module.__init__(self)\n",
    "        self.lin = t.nn.Linear(len(idx), 2) \n",
    "        self.idx = idx        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lin(x[:, self.idx])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(student, x,y):\n",
    "    student.eval()\n",
    "    total = 0 \n",
    "    correct = 0\n",
    "    with t.no_grad():\n",
    "        out = student(x)\n",
    "        correct += t.eq(t.argmax(out, 1), y).sum()\n",
    "        total+=len(x)\n",
    "    student.train()\n",
    "    return (correct/total).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение учителя\n",
    "t.manual_seed(0)\n",
    "teacher = LogReg([0,1,2])\n",
    "optim = t.optim.Adam(teacher.parameters())    \n",
    "crit = t.nn.CrossEntropyLoss()\n",
    "for e in range(10000):                                \n",
    "    teacher.zero_grad() \n",
    "    loss = crit(teacher(x_ttrain), y_ttrain)\n",
    "    loss.backward()\n",
    "    optim.step()    \n",
    "    \n",
    "    teacher.eval()\n",
    "    if e%1000==0:\n",
    "        print (accuracy(teacher, x_test, y_test ))    \n",
    "    teacher.train()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение студента без дистилляции\n",
    "student = LogReg([0,1,3])\n",
    "optim = t.optim.Adam(student.parameters())    \n",
    "crit = t.nn.CrossEntropyLoss()\n",
    "for e in range(10000):                                \n",
    "    student.zero_grad()            \n",
    "    loss = crit(student(x_train), y_train)\n",
    "    loss.backward()\n",
    "    optim.step()        \n",
    "    student.eval()\n",
    "    if e%1000==0:\n",
    "        print (accuracy(student, x_test, y_test ))    \n",
    "    student.train()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = t.nn.KLDivLoss(reduction='batchmean')\n",
    "sm = t.nn.Softmax(dim=1)\n",
    "\n",
    "def distill(out, batch_logits, temp):\n",
    "    g = sm(out/temp)\n",
    "    f = t.nn.functional.log_softmax(batch_logits/temp)    \n",
    "    return kl(f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# полная дистилляция\n",
    "beta1 = 0.0\n",
    "beta2 = 1.0\n",
    "temp = 1.0\n",
    "student = LogReg([0,1,3])\n",
    "optim = t.optim.Adam(student.parameters())    \n",
    "crit = t.nn.CrossEntropyLoss()\n",
    "teacher.eval()\n",
    "for e in range(10000):                                \n",
    "    student.zero_grad()    \n",
    "    out = student(x_train)\n",
    "    student_loss = crit(out, y_train)            \n",
    "    distillation_loss = distill(out, teacher(x_train), temp)\n",
    "    loss = beta1 * student_loss + beta2*distillation_loss        \n",
    "    loss.backward()\n",
    "    optim.step()    \n",
    "    \n",
    "    student.eval()\n",
    "    if e%1000==0:\n",
    "        print (accuracy(student, x_test, y_test ))    \n",
    "    student.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дистилляция с разными коэффициентами\n",
    "beta1 = 1.0\n",
    "beta2 = .5\n",
    "temp = 1.0\n",
    "student = LogReg([0,1,3])\n",
    "optim = t.optim.Adam(student.parameters())    \n",
    "crit = t.nn.CrossEntropyLoss()\n",
    "teacher.eval()\n",
    "for e in range(10000):                                \n",
    "    student.zero_grad()    \n",
    "    out = student(x_train)\n",
    "    student_loss = crit(out, y_train)            \n",
    "    distillation_loss = distill(out, teacher(x_train), temp)\n",
    "    loss = beta1 * student_loss + beta2*distillation_loss        \n",
    "    loss.backward()\n",
    "    optim.step()    \n",
    "    \n",
    "    student.eval()\n",
    "    if e%1000==0:\n",
    "        print (accuracy(student, x_test, y_test ))    \n",
    "    student.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_opt_full = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дистилляция с оптимизацией гиперапарметров\n",
    "\n",
    "def param_loss(batch,model,h):\n",
    "    x,y,batch_logits = batch    \n",
    "    beta,beta2,temp = h\n",
    "    out = model(x)\n",
    "    beta = F.sigmoid(beta)\n",
    "    beta2 = F.sigmoid(beta2)\n",
    "    temp = F.sigmoid(temp) * 10\n",
    "    distillation_loss = distill(out, batch_logits, temp)\n",
    "    student_loss = crit(out, y)                \n",
    "    loss = beta * distillation_loss + beta2 * student_loss\n",
    "    return loss\n",
    "\n",
    "def hyperparam_loss(batch, model):\n",
    "    x,y = batch\n",
    "    out = model(x)\n",
    "    student_loss = crit(out, y)            \n",
    "    return student_loss\n",
    "\n",
    "for _ in range(start_num):\n",
    "    results_opt = []\n",
    "\n",
    "    beta1 = t.nn.Parameter(t.tensor(np.random.uniform(low=-1, high = 1)), requires_grad=True)\n",
    "    beta2 = t.nn.Parameter(t.tensor(np.random.uniform(low=-1, high=1)), requires_grad=True)\n",
    "    temp = t.nn.Parameter(t.tensor(np.random.uniform(low=-2, high=0)), requires_grad=True)\n",
    "    h = [beta1, beta2, temp]\n",
    "\n",
    "    student = LogReg([0,1,3])\n",
    "    optim = t.optim.Adam(student.parameters())    \n",
    "    optim2 = t.optim.Adam(h,  betas=(0.5, 0.999))   \n",
    "    hyper_grad_calc = hyperparams.AdamHyperGradCalculator(student, param_loss, hyperparam_loss, optim, h)\n",
    "    crit = t.nn.CrossEntropyLoss()\n",
    "    teacher.eval()\n",
    "    for e in range(total_iteration_num):\n",
    "        \n",
    "        optim2.zero_grad()            \n",
    "        hyper_grad_calc.calc_gradients((x_train,y_train,teacher(x_train)), (x_test, y_test))\n",
    "        optim2.step()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        out = student(x_train)\n",
    "        loss = param_loss((x_train,y_train,teacher(x_train)), student,h)\n",
    "        loss.backward()\n",
    "        optim.step()  \n",
    "        student.train()       \n",
    "        if e%1000==0:\n",
    "            student.eval()\n",
    "            print(accuracy(student, x_test, y_test), float(F.sigmoid(beta1).detach().numpy()), \n",
    "                  float(F.sigmoid(beta2).detach().numpy()), \n",
    "                  float(10*F.sigmoid(temp).detach().numpy()))\n",
    "            student.train()\n",
    "            results_opt.append([e, \n",
    "                                float(accuracy(student, x_test, y_test)),\n",
    "                                float(F.sigmoid(beta1).detach().numpy()),\n",
    "                                float(F.sigmoid(beta2).detach().numpy()), \n",
    "                                float(10*F.sigmoid(temp).detach().numpy())])\n",
    "    with open('linear_exp_hyper_opt.jsonl','a') as out:\n",
    "        out.write(json.dumps(results_opt)+'\\n')\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дистилляция со сплайнами\n",
    "# параметр: сколько итераций длится одна эпоха. Обучаемся каждую вторую эпоху\n",
    "epoch_size = 10\n",
    "\n",
    "\n",
    "def param_loss(batch,model,h):\n",
    "    x,y,batch_logits = batch    \n",
    "    beta,beta2,temp = h\n",
    "    out = model(x)\n",
    "    beta = F.sigmoid(beta)\n",
    "    beta2 = F.sigmoid(beta2)\n",
    "    temp = F.sigmoid(temp) * 10\n",
    "    distillation_loss = distill(out, batch_logits, temp)\n",
    "    student_loss = crit(out, y)                \n",
    "    loss = beta * distillation_loss + beta2 * student_loss\n",
    "    return loss\n",
    "\n",
    "def hyperparam_loss(batch, model):\n",
    "    x,y = batch\n",
    "    out = model(x)\n",
    "    student_loss = crit(out, y)            \n",
    "    return student_loss\n",
    "\n",
    "\n",
    "results_opt = []\n",
    "for _ in range(start_num):\n",
    "    beta1 = t.nn.Parameter(t.tensor(np.random.uniform(low=-1, high = 1)), requires_grad=True)\n",
    "    beta2 = t.nn.Parameter(t.tensor(np.random.uniform(low=-1, high=1)), requires_grad=True)\n",
    "    temp = t.nn.Parameter(t.tensor(np.random.uniform(low=-2, high=0)), requires_grad=True)\n",
    "    h = [beta1, beta2, temp]\n",
    "\n",
    "    student = LogReg([0,1,3])\n",
    "    optim = t.optim.Adam(student.parameters())    \n",
    "    optim2 = t.optim.Adam(h,  betas=(0.5, 0.999))   \n",
    "    hyper_grad_calc = hyperparams.AdamHyperGradCalculator(student, param_loss, hyperparam_loss, optim, h)\n",
    "    crit = t.nn.CrossEntropyLoss()\n",
    "    teacher.eval()\n",
    "    for e in range(total_iteration_num):\n",
    "        e_ = e//epoch_size\n",
    "        if e%epoch_size == 0 and e_ % 2 == 0:\n",
    "                spline_hist = []\n",
    "                spline_id  = -1 \n",
    "                #optim2 = t.optim.Adam(h,  betas=(0.5, 0.999))      \n",
    "                #hyper_grad_calc = hyperparams.AdamHyperGradCalculator(student, param_loss, hyperparam_loss, optim, h)\n",
    "        # если настала пора понаблюдать за траекторий гиперпараметров\n",
    "        if  e_ % 2 == 0:           \n",
    "            optim2.zero_grad()            \n",
    "            hyper_grad_calc.calc_gradients((x_train,y_train,teacher(x_train)), (x_test, y_test))                        \n",
    "            spline_hist.append([h_.grad.cpu().detach().clone().numpy() for h_ in h])\n",
    "            optim2.step()                \n",
    "        else:\n",
    "            # иначе гиперпараметры предсказываем на основе сплайнов\n",
    "            # здесь мы делаем костыль - не даем уйти гиперпараметрам в те значения,\n",
    "            # в которых градиент потом будет нулевым                \n",
    "            spline_out = splines(spline_id)\n",
    "            optim2.zero_grad()            \n",
    "            beta1.grad.data += spline_out[0]\n",
    "            beta2.grad.data += spline_out[1]\n",
    "            temp.grad.data += spline_out[2]\n",
    "            optim2.step() \n",
    "                 \n",
    "\n",
    "\n",
    "        optim.zero_grad()\n",
    "        out = student(x_train)\n",
    "        loss = param_loss((x_train,y_train,teacher(x_train)), student,h)\n",
    "        loss.backward()\n",
    "        optim.step()  \n",
    "        student.train()       \n",
    "        if e%1000==0:\n",
    "            student.eval()\n",
    "            if e_ %2 == 0:\n",
    "                mode = 'hypertrain'\n",
    "            else:\n",
    "                mode = 'hyperpredict'\n",
    "            print(mode, accuracy(student, x_test, y_test), float(F.sigmoid(beta1).detach().numpy()), \n",
    "                  float(F.sigmoid(beta2).detach().numpy()), \n",
    "                  float(10*F.sigmoid(temp).detach().numpy()))\n",
    "            student.train()\n",
    "\n",
    "            results_opt.append([e, float(accuracy(student, x_test, y_test)), float(F.sigmoid(beta1).detach().numpy()),\n",
    "                                float(F.sigmoid(beta2).detach().numpy()), \n",
    "                                float(10*F.sigmoid(temp).detach().numpy())])\n",
    "        # если мы отслеживали траекторию эпохи - можно обучить на этом сплайны\n",
    "        if e_ % 2 == 0 and e%epoch_size == epoch_size-1:\n",
    "                fitted1 = np.polyfit(range(len(spline_hist)), np.array(spline_hist)[:,0], 1)\n",
    "                fitted2 = np.polyfit(range(len(spline_hist)), np.array(spline_hist)[:,1], 1)\n",
    "                fitted3 = np.polyfit(range(len(spline_hist)), np.array(spline_hist)[:,2], 1)\n",
    "                splines = lambda x : np.array([np.polyval(fitted1, x), np.polyval(fitted2, x), np.polyval(fitted2, x)])\n",
    "    with open('linear_exp_splines_epoch_size_{}.jsonl'.format(epoch_size),'a') as out:\n",
    "        out.write(json.dumps(results_opt)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(t.tensor(0.0))*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дистилляция со сплайнами\n",
    "# параметр: раз в сколько эпох мы обучаем сплайны\n",
    "\n",
    "train_splines_every_epoch = 10\n",
    "epoch_size = 100 # !!! размер эпохи нужно подобрать экспериментом выше\n",
    "\n",
    "\n",
    "def param_loss(batch,model,h):\n",
    "    x,y,batch_logits = batch    \n",
    "    beta,beta2,temp = h\n",
    "    out = model(x)\n",
    "    beta = F.sigmoid(beta)\n",
    "    beta2 = F.sigmoid(beta2)\n",
    "    temp = F.sigmoid(temp) * 10\n",
    "    distillation_loss = distill(out, batch_logits, temp)\n",
    "    student_loss = crit(out, y)                \n",
    "    loss = beta * distillation_loss + beta2 * student_loss\n",
    "    return loss\n",
    "\n",
    "def hyperparam_loss(batch, model):\n",
    "    x,y = batch\n",
    "    out = model(x)\n",
    "    student_loss = crit(out, y)            \n",
    "    return student_loss\n",
    "\n",
    "\n",
    "for _ in range(start_num):\n",
    "    results_opt = []\n",
    "\n",
    "    beta1 = t.nn.Parameter(t.tensor(np.random.uniform(low=-1, high = 1)), requires_grad=True)\n",
    "    beta2 = t.nn.Parameter(t.tensor(np.random.uniform(low=-1, high=1)), requires_grad=True)\n",
    "    temp = t.nn.Parameter(t.tensor(np.random.uniform(low=-2, high=0)), requires_grad=True)\n",
    "    h = [beta1, beta2, temp]\n",
    "\n",
    "    student = LogReg([0,1,3])\n",
    "    optim = t.optim.Adam(student.parameters())    \n",
    "    optim2 = t.optim.Adam(h,  betas=(0.5, 0.999))   \n",
    "    hyper_grad_calc = hyperparams.AdamHyperGradCalculator(student, param_loss, hyperparam_loss, optim, h)\n",
    "    crit = t.nn.CrossEntropyLoss()\n",
    "    teacher.eval()\n",
    "    for e in range(total_iteration_num):\n",
    "        e_ = e//epoch_size\n",
    "        if e%epoch_size == 0 and e_ % train_splines_every_epoch == 0:\n",
    "                spline_hist = []\n",
    "                spline_id  = -1                 \n",
    "        # если настала пора понаблюдать за траекторий гиперпараметров\n",
    "        if e_ % train_splines_every_epoch == 0:           \n",
    "            optim2.zero_grad()            \n",
    "            hyper_grad_calc.calc_gradients((x_train,y_train,teacher(x_train)), (x_test, y_test))            \n",
    "            optim2.step()                \n",
    "            spline_hist.append([h_.grad.cpu().detach().clone().numpy() for h_ in h])\n",
    "\n",
    "        else:\n",
    "            # иначе гиперпараметры предсказываем на основе сплайнов\n",
    "            # здесь мы делаем костыль - не даем уйти гиперпараметрам в те значения,\n",
    "            # в которых градиент потом будет нулевым                \n",
    "            spline_out = splines(spline_id)\n",
    "            optim2.zero_grad()            \n",
    "            beta1.grad.data += spline_out[0]\n",
    "            beta2.grad.data += spline_out[1]\n",
    "            temp.grad.data += spline_out[2]\n",
    "            optim2.step() \n",
    "\n",
    "\n",
    "\n",
    "        optim.zero_grad()\n",
    "        out = student(x_train)\n",
    "        loss = param_loss((x_train,y_train,teacher(x_train)), student,h)\n",
    "        loss.backward()\n",
    "        optim.step()  \n",
    "        student.train()       \n",
    "        if e%1000==0:\n",
    "            student.eval()\n",
    "            if e_ % train_splines_every_epoch == 0:\n",
    "                mode = 'hypertrain'\n",
    "            else:\n",
    "                mode = 'hyperpredict'\n",
    "            print(mode, accuracy(student, x_test, y_test), float(F.sigmoid(beta1).detach().numpy()), \n",
    "                  float(F.sigmoid(beta2).detach().numpy()), \n",
    "                  float(10*F.sigmoid(temp).detach().numpy()))\n",
    "            student.train()\n",
    "\n",
    "            results_opt.append([e, float(accuracy(student, x_test, y_test)), float(F.sigmoid(beta1).detach().numpy()),\n",
    "                                float(F.sigmoid(beta2).detach().numpy()), \n",
    "                                float(10*F.sigmoid(temp).detach().numpy())])\n",
    "            \n",
    "        # если мы отслеживали траекторию эпохи - можно обучить на этом сплайны\n",
    "        if e_ % train_splines_every_epoch == 0 and e%epoch_size == epoch_size-1:\n",
    "                fitted1 = np.polyfit(range(len(spline_hist)), np.array(spline_hist)[:,0], 1)\n",
    "                fitted2 = np.polyfit(range(len(spline_hist)), np.array(spline_hist)[:,1], 1)\n",
    "                fitted3 = np.polyfit(range(len(spline_hist)), np.array(spline_hist)[:,2], 1)\n",
    "                splines = lambda x : np.array([np.polyval(fitted1, x), np.polyval(fitted2, x), np.polyval(fitted2, x)])\n",
    "        with open('linear_exp_splines_train_every_{}.jsonl'.format(train_splines_every_epoch),'a') as out:\n",
    "            out.write(json.dumps(results_opt)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дистилляция со случайными гиперпараметрами\n",
    "# NB: здесь оставляем такую же инициализацию параметров, что и раньше! \n",
    "# это так задумано\n",
    "\n",
    "def param_loss(batch,model,h):\n",
    "    x,y,batch_logits = batch\n",
    "    #beta,temp = h\n",
    "    beta,beta2,temp = h\n",
    "    out = model(x)\n",
    "    beta = t.clamp(beta, 0.01, 0.99)\n",
    "    beta2 = t.clamp(beta2, 0.01, 0.99)\n",
    "    temp = t.clamp(temp, 0.1, 10.0)\n",
    "    distillation_loss = distill(out, batch_logits, temp)\n",
    "    student_loss = crit(out, y)            \n",
    "    #loss = (1-beta) * student_loss + beta*distillation_loss\n",
    "    loss = beta * distillation_loss + beta2 * student_loss\n",
    "    return loss\n",
    "for _ in range(start_num):\n",
    "    results_rand = []\n",
    "    results_rand_param = []\n",
    "\n",
    "    beta1 = t.nn.Parameter(t.tensor(np.random.uniform()), requires_grad=True)\n",
    "    beta2 = t.nn.Parameter(t.tensor(np.random.uniform()), requires_grad=True)\n",
    "    temp = t.nn.Parameter(t.tensor(10**np.random.uniform(low=-1, high=1)), requires_grad=True)\n",
    "    h = [beta1, beta2, temp]\n",
    "\n",
    "    student = LogReg([0,1,3])\n",
    "    optim = t.optim.Adam(student.parameters())    \n",
    "    crit = t.nn.CrossEntropyLoss()\n",
    "    teacher.eval()\n",
    "    for e in range(total_iteration_num):\n",
    "        optim.zero_grad()\n",
    "        out = student(x_train)\n",
    "        loss = param_loss((x_train,y_train,teacher(x_train)), student,h)\n",
    "        loss.backward()\n",
    "        optim.step()    \n",
    "\n",
    "        if e%1000==0:\n",
    "            student.eval()\n",
    "            print(accuracy(student, x_test, y_test), float(beta1.detach().numpy()), float(beta2.detach().numpy()), float(temp.detach().numpy()))\n",
    "            student.train()\n",
    "            results_rand.append([e, float(accuracy(student, x_test, y_test)), float(beta1.detach().numpy()), float(beta2.detach().numpy()), float(temp.detach().numpy())])\n",
    "            par = [p.detach().numpy().flatten() for p in student.parameters()]\n",
    "            results_rand_param.append(np.concatenate((par[0], par[1])))\n",
    "    with open('linear_exp_random.jsonl','a') as out:\n",
    "        out.write(json.dumps(results_rand)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rand_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.markersize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 24\n",
    "plt.rcParams['ytick.labelsize'] = 24\n",
    "plt.rcParams['legend.fontsize'] = 24\n",
    "plt.rcParams['axes.titlesize'] = 36\n",
    "plt.rcParams['axes.labelsize'] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for res in res_opt_full:\n",
    "    num+=1\n",
    "    pca = PCA(n_components=2)\n",
    "    par = pca.fit_transform(res)\n",
    "    # for i in range(3):\n",
    "    #     ax.plot(beta_h[:, i], beta2_h[:, i], temp_h[:, i], label='оптимизация гипепараметров')\n",
    "    plt.plot(par[:, 0],par[:, 1], marker='.', label=str(num))\n",
    "    plt.scatter(par[0, 0],par[0, 1], color='black')\n",
    "    \n",
    "plt.title('Дистилляция с оптимизацией\\n гиперпараметров')    \n",
    "plt.xlabel('$w_1$')\n",
    "plt.ylabel('$w_2$')\n",
    "#plt.legend()\n",
    "plt.savefig('plot_params_opt.pdf')\n",
    "#plt.show()\n",
    "\n",
    "# for angle in range(0, 360):\n",
    "#     ax.view_init(30, angle)\n",
    "#     plt.draw()\n",
    "#     plt.pause(.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(10, 10))#=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for res in res_opt_full:\n",
    "    temp = np.array(res)[:, 4]\n",
    "    beta1 = np.array(res)[:, 2]\n",
    "    beta2 = np.array(res)[:, 3]\n",
    "    # for i in range(3):\n",
    "    #     ax.plot(beta_h[:, i], beta2_h[:, i], temp_h[:, i], label='оптимизация гипепараметров')\n",
    "    ax.plot(beta1, beta2, temp, marker='.', label='оптимизация гипепараметров')\n",
    "    ax.scatter(beta1[0], beta2[0], temp[0], color='black')\n",
    "    \n",
    "ax.set_xlabel('beta1')\n",
    "ax.set_ylabel('beta2')\n",
    "ax.set_zlabel('$T_0$')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('plot_beta_temp3.pdf')\n",
    "\n",
    "# for angle in range(0, 360):\n",
    "#     ax.view_init(30, angle)\n",
    "#     plt.draw()\n",
    "#     plt.pause(.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "for res in res_full:\n",
    "    par = pca.fit_transform(res)\n",
    "    # for i in range(3):\n",
    "    #     ax.plot(beta_h[:, i], beta2_h[:, i], temp_h[:, i], label='оптимизация гипепараметров')\n",
    "    plt.plot(par[:, 0],par[:, 1], marker='.', label='')\n",
    "    plt.scatter(par[0, 0], par[0, 1], color='black')\n",
    "\n",
    "    plt.title('Дистилляция со случайными\\n значениями гиперпараметров')\n",
    "    plt.xlabel('$w_1$')\n",
    "    plt.ylabel('$w_2$')\n",
    "    #plt.legend()\n",
    "plt.savefig('plot_params_rand.pdf')\n",
    "plt.show()\n",
    "\n",
    "# for angle in range(0, 360):\n",
    "#     ax.view_init(30, angle)\n",
    "#     plt.draw()\n",
    "#     plt.pause(.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
